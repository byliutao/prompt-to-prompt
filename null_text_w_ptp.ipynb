{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcafe28c-9f59-4d96-b52f-3e3f45a16c3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Copyright 2022 Google LLC. Double-click for license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952f9ab-ed6f-4e99-8304-99a3716734b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:22:19.019606022Z",
     "start_time": "2023-09-26T03:22:18.861020084Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b343b-1c90-4747-a753-71eb7071a289",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Null-text inversion + Editing with Prompt-to-Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b6199-9dfe-4055-8a84-66ff4bfa8901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:22:22.569614294Z",
     "start_time": "2023-09-26T03:22:18.872408378Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "from IPython.display import display\n",
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import math\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import abc\n",
    "import cv2\n",
    "import ptp_utils\n",
    "import seq_aligner\n",
    "import shutil\n",
    "from torch.optim.adam import Adam\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from skimage.metrics import mean_squared_error as compare_mse\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a4bd1-3130-408b-ae2d-a166b9f19cb7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update MY_TOKEN with your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558a4b4-fec6-4bd2-9c8f-139809b1a1a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:22:29.339415358Z",
     "start_time": "2023-09-26T03:22:22.571755570Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False, steps_offset=1)\n",
    "LOW_RESOURCE = True \n",
    "NUM_DDIM_STEPS =  50 #origin: 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", local_files_only=True, scheduler=scheduler).to(device)\n",
    "try:\n",
    "    ldm_stable.disable_xformers_memory_efficient_attention()\n",
    "except AttributeError:\n",
    "    print(\"Attribute disable_xformers_memory_efficient_attention() is missing\")\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e81433eae4ee5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:23:44.556675510Z",
     "start_time": "2023-09-26T03:22:29.340846782Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "blip2_model = Blip2ForConditionalGeneration.from_pretrained(\"../model/blip2-opt-2.7b/\", torch_dtype=torch.float16).to(device)\n",
    "blip2_processor = Blip2Processor.from_pretrained(\"../model/blip2-opt-2.7b/\")\n",
    "try:\n",
    "    blip2_model.disable_xformers_memory_efficient_attention()\n",
    "except AttributeError:\n",
    "    print(\"Attribute disable_xformers_memory_efficient_attention() is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01422991-cafe-4cf0-8406-66f052a75d9b",
   "metadata": {
    "is_executing": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prompt-to-Prompt code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc083590-15de-4216-8d8d-50336f9f1d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:23:44.616237705Z",
     "start_time": "2023-09-26T03:23:44.588576293Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class LocalBlend:\n",
    "    \n",
    "    def get_mask(self, maps, alpha, use_pool):\n",
    "        k = 1\n",
    "        maps = (maps * alpha).sum(-1).mean(1)\n",
    "        if use_pool:\n",
    "            maps = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(maps, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.th[1-int(use_pool)])\n",
    "        mask = mask[:1] + mask\n",
    "        return mask\n",
    "    \n",
    "    def __call__(self, x_t, attention_store):\n",
    "        self.counter += 1\n",
    "        if self.counter > self.start_blend:\n",
    "           \n",
    "            maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "            maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "            maps = torch.cat(maps, dim=1)\n",
    "            mask = self.get_mask(maps, self.alpha_layers, True)\n",
    "            if self.substruct_layers is not None:\n",
    "                maps_sub = ~self.get_mask(maps, self.substruct_layers, False)\n",
    "                mask = mask * maps_sub\n",
    "            mask = mask.float()\n",
    "            x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], substruct_words=None, start_blend=0.2, th=(.3, .3)):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        \n",
    "        if substruct_words is not None:\n",
    "            substruct_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "            for i, (prompt, words_) in enumerate(zip(prompts, substruct_words)):\n",
    "                if type(words_) is str:\n",
    "                    words_ = [words_]\n",
    "                for word in words_:\n",
    "                    ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                    substruct_layers[i, :, :, :, :, ind] = 1\n",
    "            self.substruct_layers = substruct_layers.to(device)\n",
    "        else:\n",
    "            self.substruct_layers = None\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.start_blend = int(start_blend * NUM_DDIM_STEPS)\n",
    "        self.counter = 0 \n",
    "        self.th=th\n",
    "\n",
    "       \n",
    "class EmptyControl:\n",
    "    \n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "    \n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if LOW_RESOURCE:\n",
    "            attn = self.forward(attn, is_cross, place_in_unet)\n",
    "        else:\n",
    "            h = attn.shape[0]\n",
    "            attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class SpatialReplace(EmptyControl):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.cur_step < self.stop_inject:\n",
    "            b = x_t.shape[0]\n",
    "            x_t = x_t[:1].expand(b, *x_t.shape[1:])\n",
    "        return x_t\n",
    "\n",
    "    def __init__(self, stop_inject: float):\n",
    "        super(SpatialReplace, self).__init__()\n",
    "        self.stop_inject = int((1 - stop_inject) * NUM_DDIM_STEPS)\n",
    "        \n",
    "\n",
    "class WplusAttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_replace_index():\n",
    "        return {\"down_cross\": 0, \"mid_cross\": 0, \"up_cross\": 0,\n",
    "                \"down_self\": 0,  \"mid_self\": 0,  \"up_self\": 0}\n",
    "    \n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        # print(self.step_replace)\n",
    "        # print(\"cond:\",self.cond,\"curr_step:\",self.cur_step,\"curr_layer:\",self.cur_att_layer,\"is_cross:\",is_cross)\n",
    "        if self.cond is True: # self.cond dicide current branch\n",
    "            key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "            if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "                self.step_store[key].append(attn)\n",
    "            return attn\n",
    "        else:\n",
    "            key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "            if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "                self.step_store_uncond[key].append(attn)\n",
    "            if is_cross is False and self.cur_step < self.self_replace_steps * NUM_DDIM_STEPS:\n",
    "                # print(\"key:\",key,\"len:\",len(self.step_store[key]),\"idx:\",self.step_replace[key])\n",
    "                attn = self.step_store[key][self.step_replace[key]]\n",
    "                self.step_replace[key] += 1\n",
    "            elif is_cross is True and self.cur_step < self.cross_replace_steps * NUM_DDIM_STEPS:\n",
    "                attn = self.step_store[key][self.step_replace[key]]\n",
    "                self.step_replace[key] += 1\n",
    "            return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "\n",
    "        if len(self.attention_store_uncond) == 0:\n",
    "            self.attention_store_uncond = self.step_store_uncond\n",
    "        else:\n",
    "            for key in self.attention_store_uncond:\n",
    "                for i in range(len(self.attention_store_uncond[key])):\n",
    "                    self.attention_store_uncond[key][i] += self.step_store_uncond[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.step_store_uncond = self.get_empty_store()\n",
    "        self.step_replace = self.get_replace_index()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "    def get_average_uncond_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store_uncond[key]] for key in self.attention_store_uncond}\n",
    "        return average_attention\n",
    "\n",
    "    def reset(self):\n",
    "        super(WplusAttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.step_store_uncond = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "        self.attention_store_uncond = {}\n",
    "\n",
    "    def __init__(self, cross_replace_steps: Union[float, Tuple[float, float]], self_replace_steps: Union[float, Tuple[float, float]]):\n",
    "        super(WplusAttentionStore, self).__init__()\n",
    "        self.step_replace = self.get_replace_index()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.step_store_uncond = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "        self.attention_store_uncond = {}\n",
    "        self.cross_replace_steps = cross_replace_steps\n",
    "        self.self_replace_steps = self_replace_steps\n",
    "        self.cond = True\n",
    "        \n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace, place_in_unet):\n",
    "        if att_replace.shape[2] <= 32 ** 2:\n",
    "            attn_base = attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "            return attn_base\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce, place_in_unet)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
    "      \n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "        \n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        # attn_replace = attn_replace / attn_replace.sum(-1, keepdims=True)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
    "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
    "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
    "                  Tuple[float, ...]]):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(1, 77)\n",
    "    \n",
    "    for word, val in zip(word_select, values):\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = val\n",
    "    return equalizer\n",
    "\n",
    "def aggregate_attention(attention_store: Union[AttentionStore, WplusAttentionStore], res: int, from_where: List[str], is_cross: bool, select: int, uncond = False):\n",
    "    out = []\n",
    "    if uncond is False:\n",
    "        attention_maps = attention_store.get_average_attention()\n",
    "    else:\n",
    "        attention_maps = attention_store.get_average_uncond_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def make_controller(prompts: List[str], is_replace_controller: bool, cross_replace_steps: Dict[str, float], self_replace_steps: float, blend_words=None, equilizer_params=None) -> AttentionControlEdit:\n",
    "    if blend_words is None:\n",
    "        lb = None\n",
    "    else:\n",
    "        lb = LocalBlend(prompts, blend_word)\n",
    "    if is_replace_controller:\n",
    "        controller = AttentionReplace(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    else:\n",
    "        controller = AttentionRefine(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps, self_replace_steps=self_replace_steps, local_blend=lb)\n",
    "    if equilizer_params is not None:\n",
    "        eq = get_equalizer(prompts[1], equilizer_params[\"words\"], equilizer_params[\"values\"])\n",
    "        controller = AttentionReweight(prompts, NUM_DDIM_STEPS, cross_replace_steps=cross_replace_steps,\n",
    "                                       self_replace_steps=self_replace_steps, equalizer=eq, local_blend=lb, controller=controller)\n",
    "    return controller\n",
    "\n",
    "\n",
    "def show_cross_attention(attention_store: Union[AttentionStore, WplusAttentionStore], res: int, from_where: List[str], select: int = 0, negative_prompt = None):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "\n",
    "    if negative_prompt is not None:\n",
    "        tokens = tokenizer.encode(negative_prompt)\n",
    "        decoder = tokenizer.decode\n",
    "        attention_maps = aggregate_attention(attention_store, res, from_where, True, select, uncond = True)\n",
    "        images = []\n",
    "        for i in range(len(tokens)):\n",
    "            image = attention_maps[:, :, i]\n",
    "            image = 255 * image / image.max()\n",
    "            image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "            image = image.numpy().astype(np.uint8)\n",
    "            image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "            image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "            images.append(image)\n",
    "        ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "\n",
    "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914bda0-c191-4db6-b891-101cde74ddaf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Null Text Inversion code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442992d-8156-4dfc-a2a5-1fbf8bedb4b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:24:50.693116408Z",
     "start_time": "2023-09-26T03:24:50.256730412Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_512(image_path, left=0, right=0, top=0, bottom=0):\n",
    "    if type(image_path) is str:\n",
    "        image = np.array(Image.open(image_path))[:, :, :3]\n",
    "    else:\n",
    "        image = image_path\n",
    "    h, w, c = image.shape\n",
    "    left = min(left, w-1)\n",
    "    right = min(right, w - left - 1)\n",
    "    top = min(top, h - left - 1)\n",
    "    bottom = min(bottom, h - top - 1)\n",
    "    image = image[top:h-bottom, left:w-right]\n",
    "    h, w, c = image.shape\n",
    "    if h < w:\n",
    "        offset = (w - h) // 2\n",
    "        image = image[:, offset:offset + h]\n",
    "    elif w < h:\n",
    "        offset = (h - w) // 2\n",
    "        image = image[offset:offset + w]\n",
    "    image = np.array(Image.fromarray(image).resize((512, 512)))\n",
    "    return image\n",
    "\n",
    "\n",
    "class NullInversion:\n",
    "    \n",
    "    def prev_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * model_output\n",
    "        prev_sample = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction\n",
    "        return prev_sample\n",
    "    \n",
    "    def next_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        timestep, next_timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999), timestep\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "        next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
    "        return next_sample\n",
    "    \n",
    "    def get_noise_pred_single(self, latents, t, context):\n",
    "        noise_pred = self.model.unet(latents, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        return noise_pred\n",
    "\n",
    "    def get_noise_pred(self, latents, t, is_forward=True, context=None):\n",
    "        latents_input = torch.cat([latents] * 2)\n",
    "        if context is None:\n",
    "            context = self.context\n",
    "        guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
    "        noise_pred = self.model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "        if is_forward:\n",
    "            latents = self.next_step(noise_pred, t, latents)\n",
    "        else:\n",
    "            latents = self.prev_step(noise_pred, t, latents)\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type='np'):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.model.vae.decode(latents)['sample']\n",
    "        if return_type == 'np':\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        with torch.no_grad():\n",
    "            if type(image) is Image:\n",
    "                image = np.array(image)\n",
    "            if type(image) is torch.Tensor and image.dim() == 4:\n",
    "                latents = image\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                latents = self.model.vae.encode(image)['latent_dist'].mean\n",
    "                latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_loop(self, latent):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        all_latent = [latent]\n",
    "        latent = latent.clone().detach()\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            t = self.model.scheduler.timesteps[len(self.model.scheduler.timesteps) - i - 1]\n",
    "            noise_pred = self.get_noise_pred_single(latent, t, cond_embeddings)\n",
    "            latent = self.next_step(noise_pred, t, latent)\n",
    "            all_latent.append(latent)\n",
    "        return all_latent\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self.model.scheduler\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, image):\n",
    "        latent = self.image2latent(image)\n",
    "        image_rec = self.latent2image(latent)\n",
    "        ddim_latents = self.ddim_loop(latent)\n",
    "        return image_rec, ddim_latents\n",
    "\n",
    "    def null_optimization(self, latents, num_inner_steps, epsilon):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        uncond_embeddings_list = []\n",
    "        latent_cur = latents[-1]\n",
    "        bar = tqdm(total=num_inner_steps * NUM_DDIM_STEPS)\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            uncond_embeddings = uncond_embeddings.clone().detach()\n",
    "            uncond_embeddings.requires_grad = True\n",
    "            optimizer = Adam([uncond_embeddings], lr=1e-2 * (1. - i / 100.)) #origin: 1e-2\n",
    "            latent_prev = latents[len(latents) - i - 2]\n",
    "            t = self.model.scheduler.timesteps[i]\n",
    "            with torch.no_grad():\n",
    "                noise_pred_cond = self.get_noise_pred_single(latent_cur, t, cond_embeddings)\n",
    "            for j in range(num_inner_steps):\n",
    "                noise_pred_uncond = self.get_noise_pred_single(latent_cur, t, uncond_embeddings)\n",
    "                noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_cond - noise_pred_uncond)\n",
    "                latents_prev_rec = self.prev_step(noise_pred, t, latent_cur)\n",
    "                loss = nnf.mse_loss(latents_prev_rec, latent_prev)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_item = loss.item()\n",
    "                bar.update()\n",
    "                if loss_item < epsilon + i * 2e-5:\n",
    "                    break\n",
    "            #     print(\"step_j:\",j,\"loss:\",loss_item)\n",
    "            print(\"step_i:\",i,\"loss:\",loss_item)\n",
    "            for j in range(j + 1, num_inner_steps):\n",
    "                bar.update()\n",
    "            uncond_embeddings_list.append(uncond_embeddings[:1].detach())\n",
    "            with torch.no_grad():\n",
    "                context = torch.cat([uncond_embeddings, cond_embeddings])\n",
    "                latent_cur = self.get_noise_pred(latent_cur, t, False, context)\n",
    "        bar.close()\n",
    "        return uncond_embeddings_list\n",
    "    \n",
    "    def invert(self, image_path: str, prompt: str, offsets=(0,0,0,0), num_inner_steps=10, early_stop_epsilon=1e-5, verbose=False):\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        image_gt = load_512(image_path, *offsets)\n",
    "        if verbose:\n",
    "            print(\"DDIM inversion...\")\n",
    "        image_rec, ddim_latents = self.ddim_inversion(image_gt)\n",
    "        if verbose:\n",
    "            print(\"Null-text optimization...\")\n",
    "        uncond_embeddings = self.null_optimization(ddim_latents, num_inner_steps, early_stop_epsilon)\n",
    "        return (image_gt, image_rec), ddim_latents[-1], uncond_embeddings\n",
    "        \n",
    "    \n",
    "    def __init__(self, model):\n",
    "        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                                  set_alpha_to_one=False)\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.model.scheduler.set_timesteps(NUM_DDIM_STEPS)\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "\n",
    "null_inversion = NullInversion(ldm_stable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc843eb5a34d74",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## W Matrix Inversion code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e33c4a8d73ad89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:23:44.696475954Z",
     "start_time": "2023-09-26T03:23:44.638396971Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def w_modify_start(w_matrices, t):\n",
    "    identity_tensor = torch.ones(64).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    identity_tensor = identity_tensor.expand(1, 4, 64, 64).to(device) \n",
    "    for i in range(NUM_DDIM_STEPS):\n",
    "        if(i < NUM_DDIM_STEPS * t):\n",
    "            w_matrices[i] =  identity_tensor \n",
    "            \n",
    "def w_modify_end(w_matrices, t):\n",
    "    identity_tensor = torch.ones(64).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    identity_tensor = identity_tensor.expand(1, 4, 64, 64).to(device) \n",
    "    for i in range(NUM_DDIM_STEPS):\n",
    "        if(i > NUM_DDIM_STEPS * t):\n",
    "            w_matrices[i] =  identity_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef281540e9db85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:23:44.961596463Z",
     "start_time": "2023-09-26T03:23:44.642928740Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def circular_conv(A, B):\n",
    "    # 获取张量的形状信息\n",
    "    _, _, N = A.shape\n",
    "    \n",
    "    # 创建与 A 相同大小的结果张量\n",
    "    C = torch.zeros_like(A)\n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(N):\n",
    "            # 计算循环卷积\n",
    "            C[0, 0, n] += A[0, 0, k] * B[0, 0, (n - k) % N]\n",
    "    \n",
    "    return C\n",
    "\n",
    "def get_circulant_matrix(v):\n",
    "    assert v.shape == (1, 1, v.shape[-1]), \"Input tensor must have shape [1, 1, N]\"\n",
    "    result = torch.cat([f := v.flip(2), f[..., :-1]], dim=2).unfold(2, v.shape[-1], 1).flip(2)\n",
    "    return result\n",
    "\n",
    "def fast_circular_conv1d(a, b):\n",
    "    _, channel, size = a.shape\n",
    "    channel_results = []\n",
    "    for i in range(channel):\n",
    "        channel_a = a[:, i:i+1, :]\n",
    "        channel_b = b[:, i:i+1, :]\n",
    "        circ_mat = get_circulant_matrix(channel_a).squeeze()\n",
    "        b_mat = channel_b.squeeze().view(size,1)\n",
    "        channel_conv_result = torch.matmul(circ_mat,b_mat).view(1,1,size)\n",
    "        channel_results.append(channel_conv_result)\n",
    "    return torch.cat(channel_results, dim=1)\n",
    "\n",
    "def inverse_circular_conv1d(a, b):\n",
    "    _, channel, size = a.shape\n",
    "    channel_results = []\n",
    "    for i in range(channel):\n",
    "        channel_a = a[:, i:i+1, :]\n",
    "        channel_b = b[:, i:i+1, :]\n",
    "        circ_mat = get_circulant_matrix(channel_a).squeeze()\n",
    "        circ_mat_inv = torch.inverse(circ_mat)\n",
    "        b_mat = channel_b.squeeze().view(size,1)\n",
    "        channel_conv_result = torch.matmul(circ_mat_inv,b_mat).view(1,1,size)\n",
    "        channel_results.append(channel_conv_result)\n",
    "    return torch.cat(channel_results, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa70291efae80d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T13:02:51.878981230Z",
     "start_time": "2023-09-26T13:02:50.835815770Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#test code\n",
    "# freq_domain_noise = freq_domain_uncond + fast_circular_conv1d(freq_domain_w_matrices_cond,(freq_domain_cond - freq_domain_uncond))\n",
    "channel = 4\n",
    "size = 64\n",
    "cond = torch.rand(1,channel,size,size)\n",
    "uncond = torch.rand(1,channel,size,size)\n",
    "w = torch.rand(1,channel,size,size)\n",
    "noise = uncond + w * (cond - uncond)\n",
    "\n",
    "f_cond = torch.fft.fft(cond.view(1,channel,size*size),n=size*size)\n",
    "f_uncond = torch.fft.fft(uncond.view(1,channel,size*size),n=size*size)\n",
    "f_w = torch.fft.fft(w.view(1,channel,size*size),n=size*size)\n",
    "\n",
    "f_noise = f_uncond + fast_circular_conv1d((f_cond-f_uncond),f_w) / (size*size)\n",
    "\n",
    "noise_ = torch.fft.ifft(f_noise,n=size*size).real.view(1,channel,size,size)\n",
    "\n",
    "print(\"diff: \", torch.mean((noise - noise_) ** 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe22111cbd774b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:23:57.136810186Z",
     "start_time": "2023-09-26T03:23:45.169866654Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# channel = 4\n",
    "# size = 64\n",
    "# cond = torch.rand(1,channel,size,size)\n",
    "# uncond = torch.rand(1,channel,size,size)\n",
    "# w = torch.ones(1,channel,size,size)\n",
    "\n",
    "# f_cond = torch.fft.fft(cond.view(1,channel,size*size))\n",
    "# f_uncond = torch.fft.fft(uncond.view(1,channel,size*size))\n",
    "# f_w = torch.fft.fft(w.view(1,channel,size*size))\n",
    "\n",
    "# f_noise = f_uncond + fast_circular_conv1d((f_cond-f_uncond),f_w)\n",
    "# f_w_ = inverse_circular_conv1d((f_cond-f_uncond),(f_noise-f_uncond))\n",
    "\n",
    "# w_ = torch.fft.ifft(f_w_).real.view(1,channel,size,size)\n",
    "# print(\"diff: \", torch.mean((w - w_) ** 2)) \n",
    "# print(\"diff: \", torch.mean((f_w - f_w_) ** 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03738388cc9192b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:23:57.141753204Z",
     "start_time": "2023-09-26T03:23:57.105239280Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MatrixInversion:\n",
    "    \n",
    "    def prev_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * model_output\n",
    "        prev_sample = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction\n",
    "        return prev_sample\n",
    "    \n",
    "    def next_step(self, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "        timestep, next_timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999), timestep\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "        next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "        next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
    "        return next_sample\n",
    "    \n",
    "    def get_noise_pred_single(self, latents, t, context):\n",
    "        noise_pred = self.model.unet(latents, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        return noise_pred\n",
    "\n",
    "    def get_noise_pred(self, latents, t, is_forward=True, context=None, matrix=None):\n",
    "        latents_input = torch.cat([latents] * 2)\n",
    "        if context is None:\n",
    "            context = self.context\n",
    "        guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
    "        noise_pred = self.model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "        if matrix is None:\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "        else:\n",
    "            noise_pred = noise_pred_uncond + matrix * (noise_prediction_text - noise_pred_uncond)\n",
    "        if is_forward:\n",
    "            latents = self.next_step(noise_pred, t, latents)\n",
    "        else:\n",
    "            latents = self.prev_step(noise_pred, t, latents)\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type='np'):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.model.vae.decode(latents)['sample']\n",
    "        if return_type == 'np':\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        with torch.no_grad():\n",
    "            if type(image) is Image:\n",
    "                image = np.array(image)\n",
    "            if type(image) is torch.Tensor and image.dim() == 4:\n",
    "                latents = image\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                latents = self.model.vae.encode(image)['latent_dist'].mean\n",
    "                latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def get_prompt_embeddings(self, prompt: str):\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
    "        return text_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_loop(self, latent):\n",
    "        #get vae ddim attn and store\n",
    "        if self.use_attn_loss is True:\n",
    "            controller = AttentionStore()\n",
    "            ptp_utils.register_attention_control(self.model, controller)\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        all_latent = [latent]\n",
    "        latent = latent.clone().detach()\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            t = self.model.scheduler.timesteps[len(self.model.scheduler.timesteps) - i - 1]\n",
    "            noise_pred = self.get_noise_pred_single(latent, t, cond_embeddings)\n",
    "            latent = self.next_step(noise_pred, t, latent)\n",
    "            all_latent.append(latent)\n",
    "            attn_store = {}\n",
    "            if self.use_attn_loss is True:\n",
    "                for key, value in controller.step_store.items():\n",
    "                    if key is not 'mid_cross' and key is not 'mid_self':\n",
    "                        attn_store[key] = [v for v in value if v.shape[1]==16**2]\n",
    "                self.ddim_inv_attn.append(attn_store)\n",
    "        if self.use_attn_loss is True:\n",
    "            ptp_utils.register_attention_control(self.model, None)\n",
    "        return all_latent\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self.model.scheduler\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, image):\n",
    "        latent = self.image2latent(image)\n",
    "        image_rec = self.latent2image(latent)            \n",
    "        ddim_latents = self.ddim_loop(latent)\n",
    "        return image_rec, ddim_latents\n",
    "\n",
    "    def matrix_optimization(self, latents, num_inner_steps, epsilon, learning_rate, verbose=True):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        uncond_embeddings_list = []\n",
    "        w_matrices_list = []\n",
    "        latent_cur = latents[-1]\n",
    "        identity_tensor = torch.ones(64).unsqueeze(0).unsqueeze(0).to(self.model.device)  # Shape: (1, 1, 64, 64)\n",
    "        identity_tensor = identity_tensor.expand(1, 4, 64, 64).to(self.model.device) \n",
    "        if verbose:\n",
    "            bar = tqdm(total=num_inner_steps * NUM_DDIM_STEPS)\n",
    "        w_matrix_norm = GUIDANCE_SCALE * identity_tensor\n",
    "        lambda_norm =  self.lambda_norm\n",
    "        \n",
    "        if self.inner_steps_num is None:\n",
    "            num_inner_steps_list = np.linspace(num_inner_steps,num_inner_steps,NUM_DDIM_STEPS)\n",
    "        else:\n",
    "            num_inner_steps_list = self.inner_steps_num\n",
    "        num_inner_steps_list = [int(num) for num in num_inner_steps_list]\n",
    "        \n",
    "        if self.use_attn_loss is True:\n",
    "            controller = WplusAttentionStore(0.0,0.0)\n",
    "            ptp_utils.register_attention_control(self.model, controller)\n",
    "            \n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            curr_attn = {}\n",
    "            #TO DO: get wplus current attn\n",
    "            latent_prev = latents[len(latents) - i - 2]\n",
    "            t = self.model.scheduler.timesteps[i]\n",
    "            with torch.no_grad():\n",
    "                if self.use_attn_loss is True:\n",
    "                    controller.cond = True\n",
    "                noise_pred_cond = self.get_noise_pred_single(latent_cur, t, cond_embeddings)\n",
    "                if self.use_attn_loss is True:\n",
    "                    controller.cond = False\n",
    "                    for key, value in controller.step_store.items():\n",
    "                        if key is not 'mid_cross' and key is not 'mid_self':\n",
    "                            curr_attn[key] = [v for v in value if v.shape[1]==16**2]\n",
    "                noise_pred_uncond = self.get_noise_pred_single(latent_cur, t, uncond_embeddings) \n",
    "            \n",
    "            ## how to optimize W with attn loss, attn loss didn't influence by W, it only depend on latent_cur, t, and cond_embeddings\n",
    "            if self.use_attn_loss is True:\n",
    "                pass\n",
    "            self.cond_noises.append(noise_pred_cond)\n",
    "            self.uncond_noises.append(noise_pred_uncond)\n",
    "            \n",
    "            if self.use_freq is False:\n",
    "                w_matrices_cond = GUIDANCE_SCALE * identity_tensor\n",
    "                w_matrices_cond = w_matrices_cond.clone().detach()\n",
    "                w_matrices_cond.requires_grad = True\n",
    "                optimizer = Adam([w_matrices_cond], lr=learning_rate * (1. + 0 * (i / 20.))) ## rain\n",
    "            else:\n",
    "                w_matrices_cond = GUIDANCE_SCALE * identity_tensor\n",
    "                \n",
    "                ## convert noise_pred_start to freq space, and use (formular in freq) to computer w_freq\n",
    "                ## convert to frequency domain\n",
    "                freq_domain_cond = torch.fft.fft(noise_pred_cond.view(1,4,64*64))\n",
    "                freq_domain_uncond = torch.fft.fft(noise_pred_uncond.view(1,4,64*64))\n",
    "                freq_domian_w = torch.fft.fft(w_matrices_cond.view(1,4,64*64))\n",
    "                \n",
    "                freq_domian_noise = freq_domain_uncond + fast_circular_conv1d((freq_domain_cond-freq_domain_uncond),freq_domian_w)\n",
    "                freq_domain_w_matrices_cond = inverse_circular_conv1d((freq_domain_cond-freq_domain_uncond),(freq_domian_noise-freq_domain_uncond))\n",
    "                ##\n",
    "                \n",
    "                # freq_domain_w_matrices_cond = torch.fft.fft((GUIDANCE_SCALE * identity_tensor).view(1,4,64*64))\n",
    "                freq_domain_w_matrices_cond = freq_domain_w_matrices_cond.clone().detach()\n",
    "                freq_domain_w_matrices_cond.requires_grad = True\n",
    "                optimizer = Adam([freq_domain_w_matrices_cond], lr=learning_rate * (1. + 0 * (i / 20.))) ## rain\n",
    "\n",
    "            inner_step = num_inner_steps_list[i]\n",
    "            prev_loss = 1e100\n",
    "            for j in range(inner_step):                \n",
    "                ##1. use w+ mult frequency domain's noise\n",
    "                ##2. convert mult result to space domain\n",
    "                ##3. calculate mse_loss      \n",
    "                if self.use_freq:\n",
    "                    freq_domain_noise = freq_domain_uncond + fast_circular_conv1d(freq_domain_w_matrices_cond,(freq_domain_cond - freq_domain_uncond)) / (64*64)\n",
    "                    noise_pred = torch.real(torch.fft.ifft(freq_domain_noise)).view(1,4,64,64)\n",
    "                    w_matrices_cond = torch.real(torch.fft.ifft(freq_domain_w_matrices_cond)).view(1,4,64,64)\n",
    "                else:\n",
    "                    noise_pred = noise_pred_uncond + w_matrices_cond * (noise_pred_cond - noise_pred_uncond)\n",
    "                    # noise_pred = w_matrices_cond * noise_pred_uncond + w_fix * noise_pred_cond\n",
    "                latents_prev_rec = self.prev_step(noise_pred, t, latent_cur) #t to t-1\n",
    "                loss = nnf.mse_loss(latents_prev_rec, latent_prev) + lambda_norm * nnf.mse_loss(w_matrices_cond, w_matrix_norm)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_item = loss.item()\n",
    "                if verbose:\n",
    "                    bar.update()\n",
    "                    # print(\"step_j:\",j,\"loss:\",loss_item)\n",
    "                # if loss > prev_loss:\n",
    "                #     break\n",
    "                prev_loss = loss\n",
    "            if verbose:\n",
    "                print(\"   step_i:\",i,\"loss:\",loss_item)\n",
    "                for j in range(j + 1, num_inner_steps):\n",
    "                    bar.update()\n",
    "            uncond_embeddings_list.append(uncond_embeddings[:1].detach())\n",
    "            w_matrices_list.append(w_matrices_cond)\n",
    "            with torch.no_grad():\n",
    "                context = torch.cat([uncond_embeddings, cond_embeddings])\n",
    "                latent_cur = self.get_noise_pred(latent_cur, t, False, context, w_matrices_cond)\n",
    "        if verbose:\n",
    "            bar.close()\n",
    "        return uncond_embeddings_list, w_matrices_list\n",
    "    \n",
    "    def invert(self, image_path: str, prompt: str, offsets=(0,0,0,0),num_inner_steps=10, early_stop_epsilon=1e-5, verbose=False, learning_rate = 1e-0, verbose_bar=True):\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        image_gt = load_512(image_path, *offsets)\n",
    "        if verbose:\n",
    "            print(\"DDIM inversion...\")\n",
    "        image_rec, ddim_latents = self.ddim_inversion(image_gt)\n",
    "        if verbose:\n",
    "            print(\"Guidance Matrix optimization...\")\n",
    "        uncond_embeddings, w_matrices = self.matrix_optimization(ddim_latents, num_inner_steps, early_stop_epsilon, learning_rate, verbose=verbose_bar)\n",
    "        return (image_gt, image_rec), ddim_latents[-1], uncond_embeddings, w_matrices\n",
    "        \n",
    "    \n",
    "    def __init__(self, model, lambda_norm=1e-5, inner_steps_num=None, use_freq=False, use_attn_loss=False):\n",
    "        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                                  set_alpha_to_one=False)\n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.model.scheduler.set_timesteps(NUM_DDIM_STEPS)\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "        self.cond_noises = []\n",
    "        self.uncond_noises = []\n",
    "        self.ddim_inv_attn = []\n",
    "        self.lambda_norm = lambda_norm\n",
    "        self.inner_steps_num = inner_steps_num\n",
    "        self.use_freq = use_freq\n",
    "        self.use_attn_loss = use_attn_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0358413d59aada1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Infernce Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9499145-1a2b-4c91-900e-093c0c08043c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:23:57.142016165Z",
     "start_time": "2023-09-26T03:23:57.105700260Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def text2image_ldm_stable(\n",
    "    model,\n",
    "    prompt:  List[str],\n",
    "    controller,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: Optional[float] = 7.5,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    latent: Optional[torch.FloatTensor] = None,\n",
    "    uncond_embeddings=None,\n",
    "    optimize_matrices=None,\n",
    "    optimize_matrices_=None,\n",
    "    negative_prompt=None, # if None, another branch will be empty prompt\n",
    "    start_time=50,\n",
    "    return_type='image',\n",
    "    tao=1.0,\n",
    "    verbose_bar=True\n",
    "):\n",
    "    batch_size = len(prompt)\n",
    "    ptp_utils.register_attention_control(model, controller)\n",
    "    height = width = 512\n",
    "    \n",
    "    text_input = model.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "\n",
    "    null_input = model.tokenizer(\n",
    "        \"\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    null_embeddings = model.text_encoder(null_input.input_ids.to(model.device))[0]\n",
    "\n",
    "    if negative_prompt is not None: # negative prompt\n",
    "        uncond_input = model.tokenizer(\n",
    "        negative_prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "        uncond_embeddings_ = model.text_encoder(uncond_input.input_ids.to(model.device))[0] \n",
    "    elif uncond_embeddings is not None: # null-text optimized embedding\n",
    "        uncond_embeddings_ = None\n",
    "    else: # null embedding\n",
    "        uncond_input = model.tokenizer(\n",
    "            [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings_ = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
    "\n",
    "    latent, latents = ptp_utils.init_latent(latent, model, height, width, generator, batch_size)\n",
    "    model.scheduler.set_timesteps(num_inference_steps)\n",
    "    if verbose_bar:\n",
    "        bar = tqdm(model.scheduler.timesteps[-start_time:])\n",
    "    else:\n",
    "        bar = model.scheduler.timesteps[-start_time:]\n",
    "    for i, t in enumerate(bar):\n",
    "        if i < NUM_DDIM_STEPS * (1 - tao): #decide which step use negative prompt embedding\n",
    "            if not LOW_RESOURCE:\n",
    "                context = torch.cat([null_embeddings, text_embeddings])\n",
    "            else:\n",
    "                context = [null_embeddings, text_embeddings]\n",
    "        else:\n",
    "            if uncond_embeddings_ is None:\n",
    "                if not LOW_RESOURCE:\n",
    "                    context = torch.cat([uncond_embeddings[i].expand(*text_embeddings.shape), text_embeddings])\n",
    "                else:\n",
    "                    context = [uncond_embeddings[i].expand(*text_embeddings.shape), text_embeddings]\n",
    "            else:\n",
    "                if not LOW_RESOURCE:\n",
    "                    context = torch.cat([uncond_embeddings_, text_embeddings])\n",
    "                else:\n",
    "                    context = [uncond_embeddings_, text_embeddings]\n",
    "            \n",
    "        if (optimize_matrices is None) and (optimize_matrices_ is None): #origin\n",
    "            latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource=LOW_RESOURCE)\n",
    "        elif (optimize_matrices is not None) and (optimize_matrices_ is None): # W+\n",
    "            optimize_matrix = optimize_matrices[i].to(model.device)\n",
    "            latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale,\n",
    "             optimize_matrix=optimize_matrix, low_resource=LOW_RESOURCE)\n",
    "        elif (optimize_matrices is not None) and (optimize_matrices_ is not None): # W+ with two matrix(abondon)\n",
    "            optimize_matrix = optimize_matrices[i].to(model.device)\n",
    "            optimize_matrix_ = optimize_matrices_[i].to(model.device)\n",
    "            latents = ptp_utils.diffusion_step(model, controller, latents, context, t, guidance_scale,\n",
    "             optimize_matrix=optimize_matrix, optimize_matrix_=optimize_matrix_, low_resource=LOW_RESOURCE)\n",
    "        \n",
    "    if return_type == 'image':\n",
    "        image = ptp_utils.latent2image(model.vae, latents)\n",
    "    else:\n",
    "        image = latents\n",
    "    return image, latent\n",
    "\n",
    "\n",
    "\n",
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None, uncond_embeddings=None, optimize_matrices=None, optimize_matrices_uncond=None, negative_prompt=None, verbose=True, tao=1.0, verbose_bar=True):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DDIM_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, uncond_embeddings=uncond_embeddings, optimize_matrices=optimize_matrices, optimize_matrices_=optimize_matrices_uncond, negative_prompt=negative_prompt, tao=tao, verbose_bar=verbose_bar)\n",
    "    if verbose:\n",
    "        ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreq(image):\n",
    "    grayscale = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "    image = np.array(grayscale)\n",
    "    # 执行傅里叶变换\n",
    "    f_transform = np.fft.fft2(image)\n",
    "    \n",
    "    # 中心化频谱（将低频分量移到中心）\n",
    "    f_transform_shifted = np.fft.fftshift(f_transform)\n",
    "    \n",
    "    # 创建一个掩码，将低频分量保留下来\n",
    "    rows, cols = image.shape\n",
    "    center_row, center_col = rows // 2, cols // 2\n",
    "    mask = np.zeros((rows, cols), dtype=np.uint8)\n",
    "    mask[center_row - 30:center_row + 30, center_col - 30:center_col + 30] = 1\n",
    "    \n",
    "    # 将高频分量置零\n",
    "    f_transform_shifted *= mask\n",
    "    \n",
    "    # 逆傅里叶变换以获取低频和高频分量\n",
    "    low_frequency_component = np.fft.ifft2(np.fft.ifftshift(f_transform_shifted)).real\n",
    "    high_frequency_component = image - low_frequency_component\n",
    "    \n",
    "    return low_frequency_component.astype(int), high_frequency_component.astype(int)\n",
    "\n",
    "def rgb2ycbcr(im):\n",
    "    xform = np.array([[.299, .587, .114], [-.1687, -.3313, .5], [.5, -.4187, -.0813]])\n",
    "    ycbcr = im.dot(xform.T)\n",
    "    ycbcr[:,:,[1,2]] += 128\n",
    "    return np.uint8(ycbcr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4032faad5ecee1f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## W+ Edit Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5519c268e6f76",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### erase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f05fc699781ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:25:13.324261210Z",
     "start_time": "2023-09-26T03:24:54.611418501Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"./example_images/a black bear climb a tree in rain.png\"\n",
    "prompt = \"a black bear climb a tree in rain\"\n",
    "\n",
    "matrix_inversion = MatrixInversion(ldm_stable,inner_steps_num=None,lambda_norm=0e-7,use_freq=False,use_attn_loss=False) #derain 1e-7\n",
    "(image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True, learning_rate=1e1)\n",
    "# w_modify_end(w_matrices,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37753a2507d03f01",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-26T03:23:58.789829027Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(w_matrices[0].shape)\n",
    "for i in range(NUM_DDIM_STEPS):\n",
    "    print(\"index:\",i,\"avg:\",torch.mean(w_matrices[i]),\"max:\",torch.max(w_matrices[i]),\"min:\",torch.min(w_matrices[i]))\n",
    "    # print(w_matrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8dcb1684b48579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T03:25:42.779179891Z",
     "start_time": "2023-09-26T03:25:25.933865964Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = [prompt]\n",
    "cross_replace_steps = 0.0\n",
    "self_replace_steps = 0.0\n",
    "tao = 0.9\n",
    "negative_prompt = \"rain\"\n",
    "\n",
    "controller = WplusAttentionStore(cross_replace_steps=cross_replace_steps,self_replace_steps=self_replace_steps)\n",
    "image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, negative_prompt=negative_prompt, verbose=False, tao=tao)\n",
    "show_cross_attention(controller,16,[\"up\", \"down\"],0,negative_prompt=negative_prompt)\n",
    "controller = AttentionStore()\n",
    "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False)\n",
    "show_cross_attention(controller,16,[\"up\", \"down\"],0)\n",
    "# show_self_attention_comp(controller,16,[\"up\", \"down\"],10,0)\n",
    "print(\"showing from left to right: the ground truth image, w+ reconstruction, w+ derain\")\n",
    "ptp_utils.view_images([image_gt, image_inv[0], image_derain[0]])\n",
    "\n",
    "p = compare_psnr(image_gt, image_inv[0])\n",
    "s = compare_ssim(image_gt, image_inv[0], multichannel=True, channel_axis=2)  # 对于多通道图像(RGB、HSV等)关键词multichannel要设置为True\n",
    "m = compare_mse(image_gt, image_inv[0])\n",
    " \n",
    "print('PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6cdec19d76b114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:56:35.517487929Z",
     "start_time": "2023-09-24T07:56:35.399809345Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "# image_gt_ = Image.fromarray(image_gt)\n",
    "# image_inv_ = Image.fromarray(image_inv[0])\n",
    "# image_gt_.save(\"/home/rp/LiuTao/workspace/prompt-to-prompt/results/ground_truth.png\") \n",
    "# image_inv_.save(\"/home/rp/LiuTao/workspace/prompt-to-prompt/results/reconstruct_w+.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5c1a9f42daa08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### freq test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dda15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = compare_psnr(image_gt, image_inv[0])\n",
    "s = compare_ssim(image_gt, image_inv[0], multichannel=True, channel_axis=2)\n",
    "m = compare_mse(image_gt, image_inv[0])\n",
    " \n",
    "print('PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "y_image_gt = rgb2ycbcr(image_gt)[..., 0]\n",
    "y_image_inv = rgb2ycbcr(image_inv[0])[..., 0]\n",
    "\n",
    "p = compare_psnr(y_image_gt, y_image_inv)\n",
    "s = compare_ssim(y_image_gt, y_image_inv) \n",
    "m = compare_mse(y_image_gt, y_image_inv)\n",
    " \n",
    "print('Ycbcr PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "\n",
    "low_frequency_gt, high_frequency_gt = getFreq(image_gt)\n",
    "low_frequency_inv, high_frequency_inv = getFreq(image_inv[0])\n",
    "\n",
    "p = compare_psnr(low_frequency_gt, low_frequency_inv, data_range = low_frequency_gt.max()-low_frequency_gt.min())\n",
    "s = compare_ssim(low_frequency_gt, low_frequency_inv, data_range = low_frequency_gt.max()-low_frequency_gt.min())\n",
    "m = compare_mse(low_frequency_gt, low_frequency_inv)\n",
    "print('low_freq  PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "p = compare_psnr(high_frequency_gt, high_frequency_inv, data_range = high_frequency_gt.max()-high_frequency_gt.min())\n",
    "s = compare_ssim(high_frequency_gt, high_frequency_inv, data_range = high_frequency_gt.max()-high_frequency_gt.min())\n",
    "m = compare_mse(high_frequency_gt, high_frequency_inv)\n",
    " \n",
    "print('high_freq  PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d1ce2df22479d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764131045Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"./example_images/a red flower in rain.jpg\"\n",
    "prompt = \"a red flower in rain\"\n",
    "\n",
    "matrix_inversion = MatrixInversion(ldm_stable,inner_steps_num=None,lambda_norm=4e-6)\n",
    "(image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True, learning_rate=4e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65c491f1e6938c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764177515Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# t1 = 1.0\n",
    "# t2 = 0.8\n",
    "t1 = 0.0\n",
    "t2 = 0.0\n",
    "identity_tensor = torch.ones(64).unsqueeze(0).unsqueeze(0).to(device)\n",
    "identity_tensor = identity_tensor.expand(1, 4, 64, 64).to(device) \n",
    "for i in range(NUM_DDIM_STEPS):\n",
    "    if(i > NUM_DDIM_STEPS * t1):\n",
    "        w_matrices[i] =  identity_tensor\n",
    "    if(i < NUM_DDIM_STEPS * t2):\n",
    "        w_matrices[i] =  identity_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3a8f53deb5f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764223974Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = [prompt]\n",
    "cross_replace_steps = 0.0\n",
    "self_replace_steps = 0.0\n",
    "tao = 1.0\n",
    "negative_prompt = \"in rain\"\n",
    "\n",
    "controller = WplusAttentionStore(cross_replace_steps=cross_replace_steps,self_replace_steps=self_replace_steps)\n",
    "image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, negative_prompt=negative_prompt, verbose=False, tao=tao)\n",
    "\n",
    "controller = AttentionStore()\n",
    "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False)\n",
    "print(\"showing from left to right: the ground truth image, w+ reconstruction, w+ derain\")\n",
    "ptp_utils.view_images([image_gt, image_inv[0], image_derain[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba76b93aeb8666",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764275481Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "low_frequency_gt, high_frequency_gt = getFreq(image_gt)\n",
    "low_frequency_inv, high_frequency_inv = getFreq(image_inv[0])\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "axs[0,0].imshow(low_frequency_gt, cmap='gray', interpolation='none')\n",
    "axs[0,0].axis('off')\n",
    "axs[0,1].imshow(high_frequency_gt, cmap='gray', interpolation='none')\n",
    "axs[0,1].axis('off')\n",
    "axs[1,0].imshow(low_frequency_inv, cmap='gray', interpolation='none')\n",
    "axs[1,0].axis('off')\n",
    "axs[1,1].imshow(high_frequency_inv, cmap='gray', interpolation='none')\n",
    "axs[1,1].axis('off')\n",
    "plt.show()\n",
    "# 保存低频分量和高频分量为图像文件\n",
    "# cv2.imwrite('low_frequency_component.jpg', low_frequency_component)\n",
    "# cv2.imwrite('high_frequency_component.jpg', high_frequency_component)\n",
    "\n",
    "# print(\"Low-frequency component saved as 'low_frequency_component.jpg'\")\n",
    "# print(\"High-frequency component saved as 'high_frequency_component.jpg'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184aa270f05137c4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764368490Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## save result\n",
    "# image_gt_ = Image.fromarray(image_gt)\n",
    "# image_inv_ = Image.fromarray(image_inv[0])\n",
    "# image_gt_.save(\"/home/rp/LiuTao/workspace/prompt-to-prompt/results/ground_truth.png\") \n",
    "# image_inv_.save(\"/home/rp/LiuTao/workspace/prompt-to-prompt/results/reconstruct.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39897b03f1c2d2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### loop test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aad5fe9da5c498",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764412128Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "directory_path = \"/home/rp/LiuTao/workspace/prompt-to-prompt/example_images/loop_test/\"\n",
    "files = os.listdir(directory_path)\n",
    "matrix_inversion = MatrixInversion(ldm_stable,inner_steps_num=None,lambda_norm=0e-7)\n",
    "cross_replace_steps = 0.0\n",
    "self_replace_steps = 0.0\n",
    "tao = 1.0\n",
    "negative_prompt = \"in rain\"\n",
    "image_list = []\n",
    "for index in range(10):\n",
    "    cross_replace_steps = .0\n",
    "    self_replace_steps = index / 10\n",
    "    image_list = []\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".jpg\"):\n",
    "            image_path = directory_path + file\n",
    "            prompt = os.path.splitext(file)[0]\n",
    "            print(prompt)\n",
    "            (image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True, learning_rate=1e1)\n",
    "            \n",
    "            prompts = [prompt]\n",
    "            controller = WplusAttentionStore(cross_replace_steps=cross_replace_steps,self_replace_steps=self_replace_steps)\n",
    "            image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, negative_prompt=negative_prompt, verbose=False, tao=tao)\n",
    "            \n",
    "            controller = AttentionStore()\n",
    "            image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False)\n",
    "            # print(\"showing from left to right: the ground truth image, w+ reconstruction, w+ derain\")\n",
    "            image_list.extend([image_gt, image_inv[0], image_derain[0]])\n",
    "            # ptp_utils.view_images([image_gt, image_inv[0], image_derain[0]])\n",
    "    print(index,\":\")\n",
    "    ptp_utils.view_images(image_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229cd61eae5eb9b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764449168Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cross_replace_steps = 0.0\n",
    "self_replace_steps = 0.0\n",
    "negative_prompt = \"in rain\"\n",
    "image_list = []\n",
    "for i in range(11):\n",
    "    tao = i / 10\n",
    "    controller = WplusAttentionStore(cross_replace_steps=cross_replace_steps,self_replace_steps=self_replace_steps)\n",
    "    image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, negative_prompt=negative_prompt, verbose=False, tao=tao)\n",
    "    image_list.append(image_derain[0])\n",
    "ptp_utils.view_images(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936f0dce98f0c44",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764484017Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(1, 14):\n",
    "#     image_path = \"./example_images/A picture with raining \"+str(i)+\".jpg\"\n",
    "#     prompt = \"A picture with raining\"\n",
    "# \n",
    "#     matrix_inversion = MatrixInversion(ldm_stable,inner_steps_num=None,lambda_norm=1e-6)\n",
    "#     (image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True, learning_rate=5e0)\n",
    "# \n",
    "#     prompts = [prompt]\n",
    "#     cross_replace_steps = 0.5\n",
    "#     self_replace_steps = 0.5\n",
    "#     negative_prompt = \"with raining\"\n",
    "# \n",
    "#     controller = WplusAttentionStore(cross_replace_steps=cross_replace_steps,self_replace_steps=self_replace_steps)\n",
    "#     image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, negative_prompt=negative_prompt, verbose=False)\n",
    "# \n",
    "#     controller = AttentionStore()\n",
    "#     image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False)\n",
    "#     print(\"showing from left to right: the ground truth image, w+ reconstruction, w+ derain\")\n",
    "#     ptp_utils.view_images([image_gt, image_inv[0], image_derain[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a742a6b40d25f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-24T05:06:21.764537798Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# image_path = \"./example_images/a green tree in raining forest.png\"\n",
    "# prompt = \"a green tree in raining forest\"\n",
    "# negative_prompt = \"raining\"\n",
    "# for i in range(1,11):\n",
    "#     matrix_inversion = MatrixInversion(ldm_stable,1e-5/i)\n",
    "#     (image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True)\n",
    "#     prompts = [prompt]\n",
    "#     controller = AttentionStore()\n",
    "#     image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, negative_prompt=negative_prompt, verbose=False)\n",
    "#     image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False)\n",
    "#     print(\"lambda:\",1e-5/i)\n",
    "#     ptp_utils.view_images([image_gt, image_inv[0], image_derain[0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db469a86246cc0f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### erase with bilp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d29d747e408a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T00:00:24.227577369Z",
     "start_time": "2023-09-25T00:00:24.095851792Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img1 = load_512(\"./example_images/a bird in rain.jpg\")\n",
    "img2 = load_512(\"./example_images/a bird in rain.jpg\")\n",
    " \n",
    "p = compare_psnr(img1, img2)\n",
    "s = compare_ssim(img1, img2, multichannel=True, channel_axis=2)  # 对于多通道图像(RGB、HSV等)关键词multichannel要设置为True\n",
    "m = compare_mse(img1, img2)\n",
    " \n",
    "print('PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae5ef30757b705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:46.964293859Z",
     "start_time": "2023-09-26T13:30:37.473724289Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "matrix_inversion = MatrixInversion(ldm_stable,inner_steps_num=None,lambda_norm=0e-7,use_freq=False)\n",
    "cross_replace_steps = 0.0\n",
    "self_replace_steps = 0.0\n",
    "tao = 0.9 ##决定用多少步的negative prompt\n",
    "negative_prompt = \"rain\"\n",
    "learning_rate = 1e1\n",
    "\n",
    "directory_path_input = \"../dataset/Rain200L/test/input/\"\n",
    "directory_path_label = \"../dataset/Rain200L/test/target/\"\n",
    "directory_path_result = \"../dataset/Rain200L/test/result/\"\n",
    "directory_path_reconstruct = \"../dataset/Rain200L/test/reconstruct/\"\n",
    "suffix = \".png\"\n",
    "# prompt_prefix = \"a rainy image of \"\n",
    "# prompt_behind = \" in rain\"\n",
    "MAX_COUNT = 20\n",
    "for i in range(1,MAX_COUNT):\n",
    "    input_path = directory_path_input + str(i) + suffix\n",
    "    label_path = directory_path_label + str(i) + suffix\n",
    "    result_path = directory_path_result + str(i) + suffix\n",
    "    reconstruct_path = directory_path_reconstruct + str(i) + suffix\n",
    "    image_input = load_512(input_path)\n",
    "    image_label = load_512(label_path)\n",
    "\n",
    "    inputs = blip2_processor(images=image_input, return_tensors=\"pt\").to(device, torch.float32)\n",
    "    generated_ids = blip2_model.generate(**inputs)\n",
    "    generated_text = blip2_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    prompt =  generated_text\n",
    "    prompts = [prompt]\n",
    "\n",
    "    (image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(input_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=False, learning_rate=learning_rate, verbose_bar=False)\n",
    "    \n",
    "    controller = WplusAttentionStore(cross_replace_steps=cross_replace_steps,self_replace_steps=self_replace_steps)\n",
    "    image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, negative_prompt=negative_prompt, verbose=False, tao=tao, verbose_bar=False)\n",
    "    \n",
    "    controller = AttentionStore()\n",
    "    image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False, verbose_bar=False)\n",
    "\n",
    "    # image_inv_ = Image.fromarray(image_inv[0])\n",
    "    # image_inv_.save(reconstruct_path) \n",
    "    # image_derain_ = Image.fromarray(image_derain[0])\n",
    "    # image_derain_.save(result_path) \n",
    "\n",
    "    p = compare_psnr(image_gt, image_inv[0])\n",
    "    s = compare_ssim(image_gt, image_inv[0], multichannel=True, channel_axis=2)\n",
    "    m = compare_mse(image_gt, image_inv[0])\n",
    "    print('count: {}, prompt: {}\\nreconstruct PSNR：{}，SSIM：{}，MSE：{}'.format(i, prompt, p, s, m))\n",
    "\n",
    "    y_image_gt = rgb2ycbcr(image_gt)[..., 0]\n",
    "    y_image_inv = rgb2ycbcr(image_inv[0])[..., 0]\n",
    "\n",
    "    p = compare_psnr(y_image_gt, y_image_inv)\n",
    "    s = compare_ssim(y_image_gt, y_image_inv) \n",
    "    m = compare_mse(y_image_gt, y_image_inv)\n",
    "    \n",
    "    print('Ycbcr reconstruct PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "    p = compare_psnr(image_label, image_derain[0])\n",
    "    s = compare_ssim(image_label, image_derain[0], multichannel=True, channel_axis=2)\n",
    "    m = compare_mse(image_label, image_derain[0])\n",
    "    print('derain PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "    y_image_label = rgb2ycbcr(image_label)[..., 0]\n",
    "    y_image_derain = rgb2ycbcr(image_derain[0])[..., 0]\n",
    "\n",
    "    p = compare_psnr(y_image_label, y_image_derain)\n",
    "    s = compare_ssim(y_image_label, y_image_derain) \n",
    "    m = compare_mse(y_image_label, y_image_derain)\n",
    "    \n",
    "    print('Ycbcr derain PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "    ptp_utils.view_images([image_gt,image_inv[0],image_derain[0],image_label])\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    if i % 50 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6fe87985be79e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T08:57:21.391866246Z",
     "start_time": "2023-09-26T08:56:44.467190481Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "total_psnr_reconstruct = 0\n",
    "total_ssim_reconstruct = 0\n",
    "total_mse_reconstruct = 0\n",
    "total_psnr_derain = 0\n",
    "total_ssim_derain = 0\n",
    "total_mse_derain = 0\n",
    "total_y_psnr_reconstruct = 0\n",
    "total_y_ssim_reconstruct = 0\n",
    "total_y_mse_reconstruct = 0\n",
    "total_y_psnr_derain = 0\n",
    "total_y_ssim_derain = 0\n",
    "total_y_mse_derain = 0\n",
    "\n",
    "for i in range(1,MAX_COUNT):\n",
    "    input_path = directory_path_input + str(i) + suffix\n",
    "    label_path = directory_path_label + str(i) + suffix\n",
    "    result_path = directory_path_result + str(i) + suffix\n",
    "    reconstruct_path = directory_path_reconstruct + str(i) + suffix\n",
    "    image_input = load_512(input_path)\n",
    "    image_label = load_512(label_path)\n",
    "    image_reconstruct = load_512(reconstruct_path)\n",
    "    image_result = load_512(result_path)\n",
    "    \n",
    "    # ptp_utils.view_images([image_input,image_reconstruct,image_label,image_result])\n",
    "    p = compare_psnr(image_input, image_reconstruct)\n",
    "    s = compare_ssim(image_input, image_reconstruct, multichannel=True, channel_axis=2)\n",
    "    m = compare_mse(image_input, image_reconstruct)\n",
    "    total_psnr_reconstruct += p\n",
    "    total_ssim_reconstruct += s\n",
    "    total_mse_reconstruct += m\n",
    "    print('count: {}\\nreconstruct PSNR：{}，SSIM：{}，MSE：{}'.format(i, p, s, m))\n",
    "\n",
    "    y_image_gt = rgb2ycbcr(image_input)[..., 0]\n",
    "    y_image_inv = rgb2ycbcr(image_reconstruct)[..., 0]\n",
    "\n",
    "    p = compare_psnr(y_image_gt, y_image_inv)\n",
    "    s = compare_ssim(y_image_gt, y_image_inv) \n",
    "    m = compare_mse(y_image_gt, y_image_inv)\n",
    "    total_y_psnr_reconstruct += p\n",
    "    total_y_ssim_reconstruct += s\n",
    "    total_y_mse_reconstruct += m\n",
    "\n",
    "    \n",
    "    print('Ycbcr reconstruct PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "    p = compare_psnr(image_label, image_result)\n",
    "    s = compare_ssim(image_label, image_result, multichannel=True, channel_axis=2)\n",
    "    m = compare_mse(image_label, image_result)\n",
    "    total_psnr_derain += p\n",
    "    total_ssim_derain += s\n",
    "    total_mse_derain += m\n",
    "    print('derain PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "\n",
    "    y_image_label = rgb2ycbcr(image_label)[..., 0]\n",
    "    y_image_derain = rgb2ycbcr(image_result)[..., 0]\n",
    "\n",
    "    p = compare_psnr(y_image_label, y_image_derain)\n",
    "    s = compare_ssim(y_image_label, y_image_derain) \n",
    "    m = compare_mse(y_image_label, y_image_derain)\n",
    "    \n",
    "    print('Ycbcr derain PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))\n",
    "    total_y_psnr_derain += p\n",
    "    total_y_ssim_derain += s\n",
    "    total_y_mse_derain += m\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "print(\"\\n\")\n",
    "print('reconstruct avg_PSNR：{}，avg_SSIM：{}，avg_MSE：{}'.format(total_psnr_reconstruct/(MAX_COUNT-1), total_ssim_reconstruct/(MAX_COUNT-1), total_mse_reconstruct/(MAX_COUNT-1)))\n",
    "print('derain  avg_PSNR：{}，avg_SSIM：{}，avg_MSE：{}'.format(total_psnr_derain/(MAX_COUNT-1), total_ssim_derain/(MAX_COUNT-1), total_mse_derain/(MAX_COUNT-1)))\n",
    "print('y reconstruct avg_PSNR：{}，avg_SSIM：{}，avg_MSE：{}'.format(total_y_psnr_reconstruct/(MAX_COUNT-1), total_y_ssim_reconstruct/(MAX_COUNT-1), total_y_mse_reconstruct/(MAX_COUNT-1)))\n",
    "print('y derain  avg_PSNR：{}，avg_SSIM：{}，avg_MSE：{}'.format(total_y_psnr_derain/(MAX_COUNT-1), total_y_ssim_derain/(MAX_COUNT-1), total_y_mse_derain/(MAX_COUNT-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3369434e6f6de48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a207705d629e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:41:13.382586335Z",
     "start_time": "2023-09-14T09:40:47.438118498Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"./example_images/black and white dog playing red ball on black carpet.jpg\"\n",
    "prompt = \"black and white dog playing red ball on black carpet\"\n",
    "(image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True, learning_rate=1.2e-0)\n",
    "prompts = [prompt]\n",
    "controller = AttentionStore()\n",
    "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False)\n",
    "print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, w+ reconstruction\")\n",
    "ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "show_cross_attention(controller, 16, [\"up\", \"down\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17559c7153256ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T09:18:27.535819835Z",
     "start_time": "2023-09-12T09:18:27.494204548Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "w_matrices[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ea1b09f726e92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T09:18:28.682406370Z",
     "start_time": "2023-09-12T09:18:28.408246818Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=100000)\n",
    "print(w_matrices[0].shape)\n",
    "for i in range(1):\n",
    "    print(\"index:\",i,\"avg:\",torch.mean(w_matrices[i]),\"max:\",torch.max(w_matrices[i]),\"min:\",torch.min(w_matrices[i]))\n",
    "    print(w_matrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa59ef10e1e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T09:19:43.688150443Z",
     "start_time": "2023-09-12T09:19:32.728720810Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"black and white dog playing red ball on black carpet\",\n",
    "           \"black and white tiger playing red ball on black carpet\"\n",
    "        ]\n",
    "\n",
    "controller = AttentionStore()\n",
    "print(\"showing from left to right: the ground truth image, the w+ replace\")\n",
    "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices)\n",
    "\n",
    "print(\"Image is highly affected by the self_replace_steps, usually 0.4 is a good default value, but you may want to try the range 0.3,0.4,0.5,0.7 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80eeb45064414e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Null Text Edit Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4254ec5d433f85d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### erase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146ff4b9a26ca35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:58:35.609345297Z",
     "start_time": "2023-09-25T03:56:42.398466357Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#algorithm 1\n",
    "#input: real image and a related prompt\n",
    "#tuning the null-text embedding to learn the real image's structure by utilize the define loss function.\n",
    "#loss function is defined by real_image input, adapt ddim inversion to it, and then compare to the generated result by null_txt and prompt embedding\n",
    "#output: x_t: noise vector generate by ddim inversion , uncond_embeddings: optimized null_text embedding  \n",
    "#image_gt: input_image image_enc: apply stable diffusion model encoder and decoder to input image\n",
    "#x_t is the start point of our reconstruct\n",
    "image_path = \"./example_images/loop_test/A branch full of green leaves in the rain.jpg\"\n",
    "prompt = \"A branch full of green leaves in the rain\"\n",
    "(image_gt, image_enc), x_t, uncond_embeddings = null_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True)\n",
    "prompts = [prompt]\n",
    "controller = AttentionStore()\n",
    "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=False)\n",
    "print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text reconstruction image\")\n",
    "ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "show_cross_attention(controller, 16, [\"up\", \"down\"])\n",
    "\n",
    "p = compare_psnr(image_gt, image_inv[0])\n",
    "s = compare_ssim(image_gt, image_inv[0], multichannel=True, channel_axis=2)  # 对于多通道图像(RGB、HSV等)关键词multichannel要设置为True\n",
    "m = compare_mse(image_gt, image_inv[0])\n",
    " \n",
    "print('PSNR：{}，SSIM：{}，MSE：{}'.format(p, s, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14311321ed4d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T14:11:01.175142333Z",
     "start_time": "2023-09-22T14:10:51.514062862Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# image_inv_ = Image.fromarray(image_inv[0])\n",
    "# image_inv_.save(\"/home/rp/LiuTao/workspace/prompt-to-prompt/results/reconstruct_null.png\") \n",
    "# image_enc_ = Image.fromarray(image_enc)\n",
    "# image_enc_.save(\"/home/rp/LiuTao/workspace/prompt-to-prompt/results/reconstruct_vae.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04ec58a763bf91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T09:22:22.637340004Z",
     "start_time": "2023-09-12T09:22:06.695757100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "controller = AttentionStore()\n",
    "image_derain, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, optimize_matrices=None, negative_prompt=\"with glass\", verbose=False)\n",
    "controller = AttentionStore()\n",
    "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, optimize_matrices=None, verbose=False)\n",
    "print(\"showing from left to right: the ground truth image, null-text reconstruction, null-text derain\")\n",
    "ptp_utils.view_images([image_gt, image_inv[0], image_derain[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f70a96c6c7500",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1980df0f7c9a679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T09:24:14.657752563Z",
     "start_time": "2023-09-12T09:22:22.597096709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"./example_images/black and white dog playing red ball on black carpet.jpg\"\n",
    "prompt = \"black and white dog playing red ball on black carpet\"\n",
    "(image_gt, image_enc), x_t, uncond_embeddings = null_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=10, verbose=True)\n",
    "prompts = [prompt]\n",
    "controller = AttentionStore()\n",
    "image_inv, x_t = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=False)\n",
    "print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text inverted image, ...\")\n",
    "ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "show_cross_attention(controller, 16, [\"up\", \"down\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cb1c8484dd50e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T09:24:37.070356209Z",
     "start_time": "2023-09-12T09:24:25.874919169Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"black and white dog playing red ball on black carpet\",\n",
    "           \"black and white tiger playing red ball on black carpet\"\n",
    "        ]\n",
    "\n",
    "cross_replace_steps = {'default_': .8,}\n",
    "self_replace_steps = .5\n",
    "blend_word = ((('dog',), (\"tiger\",))) # for local edit. If it is not local yet - use only the source object: blend_word = ((('cat',), (\"cat\",))).\n",
    "eq_params = {\"words\": (\"tiger\",), \"values\": (2,)} # amplify attention to the word \"tiger\" by *2 \n",
    "\n",
    "controller = make_controller(prompts, True, cross_replace_steps, self_replace_steps, blend_word, eq_params)\n",
    "images, _ = run_and_display(prompts, controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings)\n",
    "\n",
    "# print(\"Image is highly affected by the self_replace_steps, usually 0.4 is a good default value, but you may want to try the range 0.3,0.4,0.5,0.7 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef248741b32db53e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## PSNR Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eca7cb48cdd711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T14:30:46.071013091Z",
     "start_time": "2023-09-17T14:24:14.239893917Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "matrix_inversion = MatrixInversion(ldm_stable,inner_steps_num=None,lambda_norm=1e-5)\n",
    "image_path = \"./example_images/a red flower in rain.jpg\"\n",
    "prompt = \"a red flower in rain\"\n",
    "\n",
    "w_psnr_list = []\n",
    "for i in range(1,16):\n",
    "    (image_gt, image_enc), x_t, uncond_embeddings, w_matrices = matrix_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=i, verbose=True, learning_rate=3e3)\n",
    "    prompts = [prompt]\n",
    "    w_controller = AttentionStore()\n",
    "    image_inv, x_t = run_and_display(prompts, w_controller, run_baseline=False, latent=x_t, uncond_embeddings=None, optimize_matrices=w_matrices, verbose=False)\n",
    "    # print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, w matrix optimize\")\n",
    "    # ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "    # show_cross_attention(w_controller, 16, [\"up\", \"down\"])\n",
    "    psnr = ptp_utils.PSNR(image_gt,image_inv[0])\n",
    "    w_psnr_list.append((i,psnr))\n",
    "    print(\"inner_step:\",i,\"psnr:\",psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d2b9b2c82f9c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T14:24:14.235460369Z",
     "start_time": "2023-09-17T14:04:22.576066176Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"./example_images/a red flower in rain.jpg\"\n",
    "prompt = \"a red flower in rain\"\n",
    "null_psnr_list = []\n",
    "for i in range(1,16):\n",
    "    (image_gt, image_enc), x_t, uncond_embeddings = null_inversion.invert(image_path, prompt, offsets=(0,0,0,0), num_inner_steps=i, verbose=False)\n",
    "    prompts = [prompt]\n",
    "    null_text_controller = AttentionStore()\n",
    "    image_inv, x_t = run_and_display(prompts, null_text_controller, run_baseline=False, latent=x_t, uncond_embeddings=uncond_embeddings, verbose=False)\n",
    "    # print(\"showing from left to right: the ground truth image, the vq-autoencoder reconstruction, the null-text inverted image, the inverted image without optimized null-text\")\n",
    "    # ptp_utils.view_images([image_gt, image_enc, image_inv[0]])\n",
    "    # show_cross_attention(null_text_controller, 16, [\"up\", \"down\"])\n",
    "    psnr = ptp_utils.PSNR(image_gt,image_inv[0])\n",
    "    null_psnr_list.append((i,psnr))\n",
    "    print(\"inner_step:\",i,\"psnr:\",psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f117984df93b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T14:30:46.120767944Z",
     "start_time": "2023-09-17T14:30:46.075735844Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vqae_psnr = ptp_utils.PSNR(image_gt,image_enc)\n",
    "vqae_psnr_list = []\n",
    "for i in range(len(null_psnr_list)):\n",
    "    vqae_psnr_list.append((null_psnr_list[i][0], vqae_psnr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b129ac45574bd98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T14:30:46.263841909Z",
     "start_time": "2023-09-17T14:30:46.116880152Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print(w_psnr_list,null_psnr_list)\n",
    "x_values_curve1 = [item[0] for item in w_psnr_list]\n",
    "y_values_curve1 = [item[1] for item in w_psnr_list]\n",
    "\n",
    "x_values_curve2 = [item[0] for item in null_psnr_list]\n",
    "y_values_curve2 = [item[1] for item in null_psnr_list]\n",
    "\n",
    "x_values_curve3 = [item[0] for item in vqae_psnr_list]\n",
    "y_values_curve3 = [item[1] for item in vqae_psnr_list]\n",
    "\n",
    "plt.plot(x_values_curve1, y_values_curve1, label='w+')\n",
    "plt.plot(x_values_curve2, y_values_curve2, label='null-text')\n",
    "plt.plot(x_values_curve3, y_values_curve3, label='vqae')\n",
    "\n",
    "plt.title('FIX DDIM_STEPS = ' + str(NUM_DDIM_STEPS))\n",
    "plt.xlabel('inner step')\n",
    "plt.ylabel('PSNR')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3b5f344bfb987",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-08T03:57:40.330200658Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
